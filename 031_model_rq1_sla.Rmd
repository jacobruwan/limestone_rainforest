---
title: "model_development"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Research Question 1: SLA
In this analysis, SLA is investigated to quantify the difference between dominant species and between disturbed and pristine forest conditions.

```{r load_datasets}
dropbox <- "C:\\Users\\Jacob\\Dropbox\\Semester 2 2017\\MXB344\\Project\\Data\\"

# load(paste0(dropbox, "domseedlingcover_rq1.RData"))
# load(paste0(dropbox, "domshannons_rq1.RData"))
load(paste0(dropbox, "domsla_rq1.RData"))
# load(paste0(dropbox, "species_richness_rq3.RData"))
# load(paste0(dropbox, "hobo_rq2.RData"))

# Go through the variables and unselect predictor variable that don't make sense
domsla_rq1.df %>% str()

sla.df <- domsla_rq1.df %>%
  mutate(transect = factor(transect),
         quadrat = factor(quadrat),
         dom_species_id = factor(dom_species_id)) %>%
  select(-av_dom_leaf_dry_weight,-total_dom_leaf_dry_weight,
         -start_hobo, -end_hobo, -min_rel_humid, -max_rel_humid, -species_id, -leaf_area,
         -species_id, -total_dom_leaf_area, -av_dom_sla_from_ind, -av_dom_sla_from_total) %>%
  mutate(dom_abundance = dom_abundance/max(dom_abundance))

# In the INLA model, predictions are made on a dataset with response variables as NA's
# Recreate the dataset with all NA's, then append this dataset onto the original dataset
# when performing CV OOS predictions
pred.sla.df <- sla.df %>%
  mutate(dom_sla = NA)

# Look at structure of modelling df
sla.df %>% str()
```

```{r}
# define the vector of possible predictors, exclude random effects
response <- "dom_sla"
predictors <- names(sla.df)[-(which(names(sla.df) %in% c("dom_sla", "transect", "quadrat")))]

# Iterate through code that creates all possible combinations of predictors: 7 predictors means 7^2 combinations
possible_predictor_combinations <- list()
index <- 1

for (i in 1:length(predictors)) {
  for (j in 1:length(predictors)) {
  possible_predictor_combinations[[index]] <- i:j 
  index <- index + 1
  }
}

# Create formula's
formula_string_combinations <- list()

for (i in 1:length(possible_predictor_combinations)) {
  
  formula_string_combinations[[i]] <- predictors[possible_predictor_combinations[[i]][1]]
  
  if (length(possible_predictor_combinations[[i]]) > 1) {
    for (j in 2:length(possible_predictor_combinations[[i]])) {
      formula_string_combinations[[i]] <- paste(formula_string_combinations[[i]], 
                                                predictors[possible_predictor_combinations[[i]][j]],
                                                sep = " + ")
    }
  }
    
}
```

### Bayesian Approach
```{r all_inla_model_combinations}
# Run all possible INLA models
Log_Likelihood_Summary <- data.frame(Formula = "-", Log_Likelihood = NA)

# In for loop, run the model and combine the log likelihood estimate into a data.frame
for (i in 1:length(formula_string_combinations)) {
inla_formula <- as.formula(paste0("dom_sla ~ ",
                          formula_string_combinations[[i]],
                          " + f(transect, model='iid') + f(quadrat:transect, model = 'iid')"))

fit <- inla(inla_formula,
            family = "gaussian",
            data = sla.df)

Log_Likelihood_Summary <- Log_Likelihood_Summary %>%
  rbind(data.frame(Formula = formula_string_combinations[[i]],
                   Log_Likelihood = summary(fit)[["mlik"]][2]))

}
```

```{r}
top5 <- Log_Likelihood_Summary %>%
  arrange(desc(Log_Likelihood)) %>%
  head(5)

# top5

# RUn a model on the top 3 variables identified
inla_formula <- as.formula("dom_sla ~ site + dom_species_id + dom_abundance + f(transect, model='iid') + f(quadrat:transect, model = 'iid')")

fit <- inla(inla_formula,
            family = "gaussian",
            data = sla.df)

# analyse the output of the model
summary(fit)

# So SiteP and av_quad_compaction both contain 0 in their credible intervals..
# Only dominant species id demonstrates any significant difference
# expected result from exploratory plots
```

#### OOS Predictions
```{r}
# set.seed(220713)
# inTest <- sample(nrow(sla.df), 40)
# predictions.df <- rbind(sla.df[-inTest,], pred.sla.df[inTest,])

inla_formula <- as.formula("dom_sla ~ site + site:dom_species_id + dom_species_id + dom_abundance + f(transect, model='iid') + f(quadrat:transect, model = 'iid')")

fit <- inla(inla_formula,
            family = "gaussian",
            data = sla.df,
            control.compute = list(cpo=TRUE),
            control.inla = list(strategy = "laplace", 
                                npoints = 21))

summary(fit)

df <- cbind(sla.df,
      data.frame(dom_sla_pred = fit$summary.fitted.values$mean)) %>%
  tbl_df()

df %>%
  ggplot() +
  geom_point(aes(x = dom_sla, y = dom_sla_pred)) +
  xlab("Observed SLA") +
  ylab("Predicted SLA") +
  theme_project()

# QQ-Plot

# 
qqnorm(fit$cpo$pit)
qqline(fit$cpo$pit)

# Histogram of predictions
df %>%
  ggplot(aes(x = dom_sla_pred)) +
  geom_histogram()
  


```

#### Parameter Credible Intervals
```{r }
## How to look at predictions and compare to original???
fixed <- summary(fit)$fixed
vec <- fixed[1:length(fixed)]
# random <- summary(fit)$random
n <- length(summary(fit)$fixed)
n1 <- n/7

# Collect and plot the intercals
# How to standardise the intervals?
# Standardise the response variable?
fixed.df <- data.frame(Estimate = row.names(fixed),
           Mean = vec[1:n1],
           SD = vec[(n1+1):(2*n1)],
           quant0.025 = vec[(2*n1+1):(3*n1)],
           quant0.5 = vec[(3*n1+1):(4*n1)],
           quant0.975 = vec[(4*n1+1):(5*n1)],
           mode = vec[(5*n1+1):(6*n1)])

fixed.df %>%
  ggplot(aes(y = Estimate, x = quant0.5, xmin = quant0.025, xmax = quant0.975)) +
  geom_errorbarh() +
  theme_project() +
  ggtitle("Standardised Predictors: Parameter Estimate Confidence Intervals") +
  xlab("Median")
```


#### Model Fit
```{r}
# cpo
# pit
#


```


### Frequentist Approach

```{r}
# Try the same model using glmer
# Run all possible glmer models
Log_Likelihood_Summary2 <- data.frame(Formula = "-", Log_Likelihood = NA)

# In for loop, run the model and combine the log likelihood estimate into a data.frame
for (i in 1:length(formula_string_combinations)) {
glmm_formula <- as.formula(paste0("dom_sla ~ ",
                          formula_string_combinations[[i]],
                          " + (1|transect/quadrat)"))

fit2 <- glmer(glmm_formula,
             data = sla.df,
             family = "gaussian")

Log_Likelihood_Summary2 <- Log_Likelihood_Summary2 %>%
  rbind(data.frame(Formula = formula_string_combinations[[i]],
                   Log_Likelihood = summary(fit2)[["logLik"]][1]))

}


```

```{r}
top5 <- Log_Likelihood_Summary2 %>%
  arrange(desc(Log_Likelihood)) %>%
  head(5)

# top5

# So all seven variables in the model minimises the log likelihood.. great..

# RUn a model and investigate the estimate confidence intervals

fit2 <- glmer(dom_sla ~ site + dom_species_id + dom_abundance + dom_seedling_cover + av_quad_compaction + av_trans_elevation + av_trans_decomposition + min_temp + max_temp + (1|transect/quadrat),
             data = sla.df,
             family = "gaussian")

confint(fit2)

# Once again, only dom_species_id excludes 0 in the confidence interval
# Dom species 2 and 3 overlapp however, demonstrating no significant difference between each other
```

